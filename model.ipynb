{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset and Train Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Load the dataset from a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73abdf5e303c4a4d837121a4695b250c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'entities'],\n",
      "        num_rows: 148500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from a JSONL file\n",
    "dataset = load_dataset(\"json\", data_files=\"data/ner_dataset.jsonl\")\n",
    "\n",
    "# Check the structure\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset\n",
    "\n",
    "Reserve 10% for model testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1)  # 10% for testing\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6fa4ce414374d05be8b54ee5f45f0ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/133650 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f514f22853943a2abc0b232683a93ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '15:13:21 ERROR Invalid API key used for request to dehmel.info.', 'entities': [{'start': 51, 'end': 62, 'label': 'DOMAIN'}], 'input_ids': [0, 996, 35, 1558, 35, 2146, 38586, 38539, 21013, 762, 341, 13, 2069, 7, 263, 298, 17170, 4, 23999, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'text': 'http://reyes.com/list/search/wp-content?hand=96&physical=24&where=92 is your gateway to cutting-edge solutions.', 'entities': [{'start': 0, 'end': 68, 'label': 'URL'}], 'input_ids': [0, 8166, 640, 5460, 293, 4, 175, 73, 8458, 73, 21061, 73, 12251, 12, 10166, 116, 4539, 5214, 5607, 947, 41359, 5214, 1978, 947, 8569, 5214, 6617, 16, 110, 23247, 7, 3931, 12, 14724, 2643, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "\n",
    "import token\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "label_map = {\n",
    "    \"O\": 0,  # \"O\" stands for \"Outside\" (no entity)\n",
    "    \"COMPANY\": 1,\n",
    "    \"DOMAIN\": 2,\n",
    "    \"IP_ADDR\": 3,\n",
    "    \"URL\": 4,\n",
    "    \"EMAIL\": 5,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128, return_offsets_mapping=True\n",
    "    )\n",
    "    labels = []\n",
    "    for i, offsets in enumerate(tokenized_inputs[\"offset_mapping\"]):\n",
    "        entity_positions = examples[\"entities\"][i]\n",
    "        label_ids = [label_map[\"O\"]] * len(offsets)  # Initialize all tokens as \"O\"\n",
    "        for entity in entity_positions:\n",
    "            for idx, (start, end) in enumerate(offsets):\n",
    "                if start >= entity[\"start\"] and end <= entity[\"end\"]:\n",
    "                    label_ids[idx] = label_map[entity[\"label\"]]  # Use numeric ID for the label\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    tokenized_inputs.pop(\"offset_mapping\")  # Remove offset mapping as itâ€™s not needed for training\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply tokenization in batches\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "print(tokenized_train_dataset[0])\n",
    "print(tokenized_test_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/1n/tq5jk8rd6ld4v4237b0vc7v40000gn/T/ipykernel_51295/2311080359.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1316db4372be49f8808f4eedff17449d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25062 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0642, 'grad_norm': 0.01806827262043953, 'learning_rate': 1.9600989545926105e-05, 'epoch': 0.06}\n",
      "{'loss': 0.0024, 'grad_norm': 0.056169819086790085, 'learning_rate': 1.9201979091852208e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0053, 'grad_norm': 0.0017644035397097468, 'learning_rate': 1.880296863777831e-05, 'epoch': 0.18}\n",
      "{'loss': 0.0049, 'grad_norm': 0.007803365122526884, 'learning_rate': 1.8403958183704415e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0028, 'grad_norm': 0.0019374164985492826, 'learning_rate': 1.8004947729630518e-05, 'epoch': 0.3}\n",
      "{'loss': 0.0005, 'grad_norm': 4.200196266174316, 'learning_rate': 1.760593727555662e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0008, 'grad_norm': 0.0004985977429896593, 'learning_rate': 1.7206926821482724e-05, 'epoch': 0.42}\n",
      "{'loss': 0.0006, 'grad_norm': 0.0005463177803903818, 'learning_rate': 1.6807916367408827e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0005226978682912886, 'learning_rate': 1.640890591333493e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0009, 'grad_norm': 0.004539235960692167, 'learning_rate': 1.6009895459261034e-05, 'epoch': 0.6}\n",
      "{'loss': 0.0012, 'grad_norm': 0.0007166142459027469, 'learning_rate': 1.5610885005187137e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0026, 'grad_norm': 0.00038105883868411183, 'learning_rate': 1.5211874551113242e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0023, 'grad_norm': 0.0008639098959974945, 'learning_rate': 1.4812864097039344e-05, 'epoch': 0.78}\n",
      "{'loss': 0.0021, 'grad_norm': 0.0004111869784537703, 'learning_rate': 1.4413853642965447e-05, 'epoch': 0.84}\n",
      "{'loss': 0.001, 'grad_norm': 0.0003953234408982098, 'learning_rate': 1.401484318889155e-05, 'epoch': 0.9}\n",
      "{'loss': 0.001, 'grad_norm': 0.00011632806854322553, 'learning_rate': 1.3615832734817653e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b034b30f444f919ce3208f6f38c31d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1857 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.002017647260800004, 'eval_runtime': 62.4992, 'eval_samples_per_second': 237.603, 'eval_steps_per_second': 29.712, 'epoch': 1.0}\n",
      "{'loss': 0.0015, 'grad_norm': 0.000349904817994684, 'learning_rate': 1.3216822280743758e-05, 'epoch': 1.02}\n",
      "{'loss': 0.0018, 'grad_norm': 0.00033905485179275274, 'learning_rate': 1.281781182666986e-05, 'epoch': 1.08}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0025566082913428545, 'learning_rate': 1.2418801372595963e-05, 'epoch': 1.14}\n",
      "{'loss': 0.0009, 'grad_norm': 0.0017402688972651958, 'learning_rate': 1.2019790918522068e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0009, 'grad_norm': 0.0007227610913105309, 'learning_rate': 1.162078046444817e-05, 'epoch': 1.26}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0016576049383729696, 'learning_rate': 1.1221770010374273e-05, 'epoch': 1.32}\n",
      "{'loss': 0.0021, 'grad_norm': 0.0005424692062661052, 'learning_rate': 1.0822759556300376e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0009, 'grad_norm': 0.000179406299139373, 'learning_rate': 1.0423749102226479e-05, 'epoch': 1.44}\n",
      "{'loss': 0.0023, 'grad_norm': 0.00010763546742964536, 'learning_rate': 1.0024738648152584e-05, 'epoch': 1.5}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0007169608725234866, 'learning_rate': 9.625728194078686e-06, 'epoch': 1.56}\n",
      "{'loss': 0.0, 'grad_norm': 0.001626434619538486, 'learning_rate': 9.226717740004789e-06, 'epoch': 1.62}\n",
      "{'loss': 0.0011, 'grad_norm': 0.00014466617722064257, 'learning_rate': 8.827707285930892e-06, 'epoch': 1.68}\n",
      "{'loss': 0.0019, 'grad_norm': 0.00029076149803586304, 'learning_rate': 8.428696831856995e-06, 'epoch': 1.74}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0031650797463953495, 'learning_rate': 8.029686377783098e-06, 'epoch': 1.8}\n",
      "{'loss': 0.0009, 'grad_norm': 6.949947419343516e-05, 'learning_rate': 7.630675923709202e-06, 'epoch': 1.86}\n",
      "{'loss': 0.0001, 'grad_norm': 9.12548421183601e-05, 'learning_rate': 7.231665469635305e-06, 'epoch': 1.92}\n",
      "{'loss': 0.0001, 'grad_norm': 9.535274875815958e-05, 'learning_rate': 6.832655015561408e-06, 'epoch': 1.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea31c85dc1534d1d846663c1e2273b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1857 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0010500079952180386, 'eval_runtime': 61.9284, 'eval_samples_per_second': 239.793, 'eval_steps_per_second': 29.986, 'epoch': 2.0}\n",
      "{'loss': 0.0, 'grad_norm': 0.0009540645405650139, 'learning_rate': 6.433644561487512e-06, 'epoch': 2.03}\n",
      "{'loss': 0.001, 'grad_norm': 7.870363333495334e-05, 'learning_rate': 6.034634107413615e-06, 'epoch': 2.09}\n",
      "{'loss': 0.0001, 'grad_norm': 5.6935397878987715e-05, 'learning_rate': 5.635623653339718e-06, 'epoch': 2.15}\n",
      "{'loss': 0.0, 'grad_norm': 7.448661199305207e-05, 'learning_rate': 5.236613199265821e-06, 'epoch': 2.21}\n",
      "{'loss': 0.0, 'grad_norm': 3.687303978949785e-05, 'learning_rate': 4.837602745191924e-06, 'epoch': 2.27}\n",
      "{'loss': 0.001, 'grad_norm': 0.0019951483700424433, 'learning_rate': 4.4385922911180275e-06, 'epoch': 2.33}\n",
      "{'loss': 0.0, 'grad_norm': 5.876285649719648e-05, 'learning_rate': 4.039581837044131e-06, 'epoch': 2.39}\n",
      "{'loss': 0.0009, 'grad_norm': 0.00011076569353463128, 'learning_rate': 3.6405713829702344e-06, 'epoch': 2.45}\n",
      "{'loss': 0.0, 'grad_norm': 4.773002001456916e-05, 'learning_rate': 3.241560928896337e-06, 'epoch': 2.51}\n",
      "{'loss': 0.0, 'grad_norm': 2.9907772841397673e-05, 'learning_rate': 2.842550474822441e-06, 'epoch': 2.57}\n",
      "{'loss': 0.0009, 'grad_norm': 0.0027552794199436903, 'learning_rate': 2.4435400207485437e-06, 'epoch': 2.63}\n",
      "{'loss': 0.0, 'grad_norm': 6.752768967999145e-05, 'learning_rate': 2.044529566674647e-06, 'epoch': 2.69}\n",
      "{'loss': 0.0001, 'grad_norm': 4.018561958218925e-05, 'learning_rate': 1.6455191126007503e-06, 'epoch': 2.75}\n",
      "{'loss': 0.0, 'grad_norm': 3.8896079786354676e-05, 'learning_rate': 1.2465086585268536e-06, 'epoch': 2.81}\n",
      "{'loss': 0.0, 'grad_norm': 3.342383570270613e-05, 'learning_rate': 8.474982044529567e-07, 'epoch': 2.87}\n",
      "{'loss': 0.0018, 'grad_norm': 0.00012239704665262252, 'learning_rate': 4.4848775037905995e-07, 'epoch': 2.93}\n",
      "{'loss': 0.0015, 'grad_norm': 0.0020557655952870846, 'learning_rate': 4.9477296305163196e-08, 'epoch': 2.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126cafb3a9ba4a75800d9c1e9f1d0b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1857 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0004938127822242677, 'eval_runtime': 62.5652, 'eval_samples_per_second': 237.352, 'eval_steps_per_second': 29.681, 'epoch': 3.0}\n",
      "{'train_runtime': 5793.2871, 'train_samples_per_second': 69.209, 'train_steps_per_second': 4.326, 'train_loss': 0.0022982391786587892, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=25062, training_loss=0.0022982391786587892, metrics={'train_runtime': 5793.2871, 'train_samples_per_second': 69.209, 'train_steps_per_second': 4.326, 'total_flos': 2.61926808447744e+16, 'train_loss': 0.0022982391786587892, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"roberta-base\", num_labels=len(label_map)\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models/ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"logs\",\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    tokenizer=tokenizer,    \n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/ner/tokenizer_config.json',\n",
       " 'models/ner/special_tokens_map.json',\n",
       " 'models/ner/vocab.json',\n",
       " 'models/ner/merges.txt',\n",
       " 'models/ner/added_tokens.json',\n",
       " 'models/ner/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"models/ner\")\n",
    "tokenizer.save_pretrained(\"models/ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Model to ONNX\n",
    "\n",
    "Use:\n",
    "\n",
    "```\n",
    "optimum-cli export onnx --model models/ner --task token-classification models/onnx\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Exported Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Model Outputs: [array([[[ 4.4469266 ,  2.2834892 ,  1.2277882 , -3.4402235 ,\n",
      "         -2.331243  , -3.9127386 ],\n",
      "        [ 7.983002  , -0.6029357 , -0.89352286, -3.0848005 ,\n",
      "         -1.571463  , -3.4708927 ],\n",
      "        [ 4.613029  ,  1.105935  ,  1.8005015 , -3.7764654 ,\n",
      "         -2.3312867 , -3.7697682 ],\n",
      "        [ 3.2291615 ,  2.4427762 ,  2.321777  , -3.7389376 ,\n",
      "         -2.4622188 , -3.6951158 ],\n",
      "        [ 2.5670388 ,  3.318974  ,  2.627408  , -3.634867  ,\n",
      "         -2.5895925 , -3.8985283 ],\n",
      "        [ 3.1018884 ,  3.634954  ,  2.065163  , -3.442695  ,\n",
      "         -2.5756822 , -4.1144876 ]]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import torch\n",
    "import onnxruntime as ort\n",
    "\n",
    "onnx_model_path = \"models/onnx/model.onnx\"\n",
    "\n",
    "# Load and check the ONNX model\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "# Run inference with ONNX Runtime\n",
    "session = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "# Prepare dummy input\n",
    "input_ids = [[101, 2054, 2003, 1996, 2171, 102]]  # Token IDs (example)\n",
    "attention_mask = [[1, 1, 1, 1, 1, 1]]  # Attention mask\n",
    "\n",
    "inputs = {\n",
    "    \"input_ids\": torch.tensor(input_ids, dtype=torch.long).numpy(),\n",
    "    \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long).numpy(),\n",
    "}\n",
    "\n",
    "# Run the ONNX model\n",
    "outputs = session.run(None, inputs)\n",
    "print(\"ONNX Model Outputs:\", outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model with Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "714f48b627cd47f896b686421f0fec7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1n/tq5jk8rd6ld4v4237b0vc7v40000gn/T/ipykernel_51295/3048475026.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73cb7b87852442a9ecade23b38164b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1857 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0004938127822242677, 'eval_model_preparation_time': 0.0014, 'eval_precision': 0.9995323743458274, 'eval_recall': 0.9999247970394518, 'eval_f1': 0.9997282489155647, 'eval_runtime': 83.4981, 'eval_samples_per_second': 177.848, 'eval_steps_per_second': 22.24}\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Tokenized test dataset (ensure it's already tokenized)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Define the label map and ensure keys are plain integers\n",
    "id2label = {int(v): k for k, v in label_map.items()}\n",
    "label2id = {k: int(k) for k, v in id2label.items()}\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (-100) from predictions and labels\n",
    "    true_predictions = [\n",
    "        [id2label[int(p)] for (p, l) in zip(pred, label) if l != -100]\n",
    "        for pred, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[int(l)] for (p, l) in zip(pred, label) if l != -100]\n",
    "        for pred, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    # Use sklearn's classification report for detailed metrics\n",
    "    results = classification_report(\n",
    "        [item for sublist in true_labels for item in sublist],\n",
    "        [item for sublist in true_predictions for item in sublist],\n",
    "        output_dict=True,\n",
    "    )\n",
    "    return {\n",
    "        \"precision\": results[\"macro avg\"][\"precision\"],\n",
    "        \"recall\": results[\"macro avg\"][\"recall\"],\n",
    "        \"f1\": results[\"macro avg\"][\"f1-score\"],\n",
    "    }\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"models/ner\")\n",
    "\n",
    "# Define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "results = trainer.evaluate()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'COMPANY', 'score': np.float32(0.731601), 'word': ' Monkres', 'start': 12, 'end': 19}, {'entity_group': 'COMPANY', 'score': np.float32(0.99096173), 'word': ' ABC Corp', 'start': 29, 'end': 37}]\n"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForTokenClassification\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "onnx_path = \"models/onnx\"\n",
    "trained_model_path = \"models/ner\"\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = ORTModelForTokenClassification.from_pretrained(onnx_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(trained_model_path)\n",
    "\n",
    "# Define the label map\n",
    "label_map = {0: \"O\", 1: \"COMPANY\", 2: \"DOMAIN\", 3: \"IP_ADDR\", 4: \"URL\", 5: \"EMAIL\"}\n",
    "\n",
    "# Update the model's config with the label map\n",
    "onnx_model.config.id2label = label_map\n",
    "onnx_model.config.label2id = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# Create a pipeline for NER\n",
    "onnx_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=onnx_model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "# Test inference\n",
    "text = \"Dr. Lynn H. Monkres works at ABC Corp.\"\n",
    "results = onnx_pipeline(text)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (881 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/6...\n",
      "Processing chunk 2/6...\n",
      "Processing chunk 3/6...\n",
      "Processing chunk 4/6...\n",
      "Processing chunk 5/6...\n",
      "Processing chunk 6/6...\n",
      "{'entity_group': 'COMPANY', 'score': np.float32(0.9177847), 'word': ' Zenith Solutions', 'start': 307, 'end': 323}\n",
      "{'entity_group': 'COMPANY', 'score': np.float32(0.75225896), 'word': ' NovaSphere Industries', 'start': 325, 'end': 346}\n",
      "{'entity_group': 'COMPANY', 'score': np.float32(0.7816794), 'word': ' Bluebridge Analytics', 'start': 352, 'end': 372}\n",
      "{'entity_group': 'COMPANY', 'score': np.float32(0.96439725), 'word': ' Vertex Tech Labs', 'start': 400, 'end': 416}\n",
      "{'entity_group': 'COMPANY', 'score': np.float32(0.9970113), 'word': ' AstraCore Innovations', 'start': 421, 'end': 442}\n",
      "{'entity_group': 'COMPANY', 'score': np.float32(0.99282616), 'word': ' Zenith Solutions', 'start': 748, 'end': 764}\n",
      "{'entity_group': 'COMPANY', 'score': np.float32(0.83525735), 'word': ' NovaSphere Industries', 'start': 858, 'end': 879}\n",
      "{'entity_group': 'COMPANY', 'score': np.float32(0.77063876), 'word': ' Vertex Tech Labs', 'start': 1001, 'end': 1017}\n",
      "{'entity_group': 'COMPANY', 'score': np.float32(0.9991107), 'word': ' NovaSphere Industries', 'start': 76, 'end': 97}\n",
      "{'entity_group': 'COMPANY', 'score': np.float32(0.98718894), 'word': ' Vertex Tech Labs', 'start': 219, 'end': 235}\n",
      "{'entity_group': 'COMPANY', 'score': np.float32(0.95539814), 'word': ' Bluebridge Analytics', 'start': 507, 'end': 527}\n",
      "{'entity_group': 'COMPANY', 'score': np.float32(0.99046105), 'word': '\\nZenith Solutions', 'start': 225, 'end': 242}\n",
      "{'entity_group': 'COMPANY', 'score': np.float32(0.99856097), 'word': 'NovaSphere Industries', 'start': 353, 'end': 374}\n",
      "{'entity_group': 'COMPANY', 'score': np.float32(0.9936533), 'word': 'Bluebridge Analytics', 'start': 481, 'end': 501}\n",
      "{'entity_group': 'DOMAIN', 'score': np.float32(0.998285), 'word': 'www.marketpulse', 'start': 291, 'end': 306}\n",
      "{'entity_group': 'DOMAIN', 'score': np.float32(0.99951637), 'word': 'com', 'start': 307, 'end': 310}\n",
      "{'entity_group': 'URL', 'score': np.float32(0.91989994), 'word': ' https://', 'start': 362, 'end': 370}\n",
      "{'entity_group': 'DOMAIN', 'score': np.float32(0.92794627), 'word': 'reports.novasphere', 'start': 370, 'end': 388}\n",
      "{'entity_group': 'DOMAIN', 'score': np.float32(0.9996393), 'word': 'com', 'start': 389, 'end': 392}\n",
      "{'entity_group': 'URL', 'score': np.float32(0.8275694), 'word': '/q', 'start': 392, 'end': 394}\n",
      "{'entity_group': 'URL', 'score': np.float32(0.96049535), 'word': '-', 'start': 395, 'end': 396}\n",
      "{'entity_group': 'IP_ADDR', 'score': np.float32(0.99919605), 'word': ' 203.0.113.42', 'start': 609, 'end': 621}\n",
      "{'entity_group': 'DOMAIN', 'score': np.float32(0.9997466), 'word': 'zenithsolutions.com', 'start': 716, 'end': 735}\n",
      "{'entity_group': 'DOMAIN', 'score': np.float32(0.99980253), 'word': 'zenithsolutions', 'start': 82, 'end': 97}\n",
      "{'entity_group': 'DOMAIN', 'score': np.float32(0.99975187), 'word': 'com', 'start': 98, 'end': 101}\n",
      "{'entity_group': 'IP_ADDR', 'score': np.float32(0.99988335), 'word': ' 2001:0db8:85a3:0000:0000:8a2e:0370:7334', 'start': 351, 'end': 390}\n",
      "{'entity_group': 'EMAIL', 'score': np.float32(0.9991089), 'word': ' rmcadams@', 'start': 537, 'end': 546}\n",
      "{'entity_group': 'DOMAIN', 'score': np.float32(0.99977297), 'word': 'zenithsolutions', 'start': 546, 'end': 561}\n",
      "{'entity_group': 'DOMAIN', 'score': np.float32(0.99964666), 'word': 'com', 'start': 562, 'end': 565}\n",
      "{'entity_group': 'EMAIL', 'score': np.float32(0.99916965), 'word': ' jcarter@', 'start': 600, 'end': 608}\n",
      "{'entity_group': 'DOMAIN', 'score': np.float32(0.9998155), 'word': 'bluebridgeanalytics', 'start': 608, 'end': 627}\n",
      "{'entity_group': 'DOMAIN', 'score': np.float32(0.99976546), 'word': 'com', 'start': 628, 'end': 631}\n",
      "{'entity_group': 'COMPANY', 'score': np.float32(0.6146859), 'word': 'raCore Innovations', 'start': 677, 'end': 695}\n",
      "{'entity_group': 'URL', 'score': np.float32(0.85989577), 'word': '://', 'start': 702, 'end': 705}\n",
      "{'entity_group': 'DOMAIN', 'score': np.float32(0.9969332), 'word': 'astracore', 'start': 705, 'end': 714}\n",
      "{'entity_group': 'DOMAIN', 'score': np.float32(0.99926025), 'word': 'com', 'start': 715, 'end': 718}\n",
      "{'entity_group': 'URL', 'score': np.float32(0.64872086), 'word': '/', 'start': 718, 'end': 719}\n",
      "{'entity_group': 'DOMAIN', 'score': np.float32(0.99340314), 'word': ' securehosting.net', 'start': 756, 'end': 773}\n",
      "{'entity_group': 'DOMAIN', 'score': np.float32(0.9999046), 'word': 'bluebridgeanalytics.com', 'start': 0, 'end': 23}\n",
      "{'entity_group': 'DOMAIN', 'score': np.float32(0.55612636), 'word': ' Astra', 'start': 66, 'end': 71}\n",
      "{'entity_group': 'COMPANY', 'score': np.float32(0.5471648), 'word': 'Core', 'start': 71, 'end': 75}\n",
      "{'entity_group': 'DOMAIN', 'score': np.float32(0.43764657), 'word': ' Innov', 'start': 76, 'end': 81}\n",
      "{'entity_group': 'URL', 'score': np.float32(0.9452664), 'word': ' https://', 'start': 89, 'end': 97}\n",
      "{'entity_group': 'DOMAIN', 'score': np.float32(0.99920696), 'word': 'astracore', 'start': 97, 'end': 106}\n",
      "{'entity_group': 'DOMAIN', 'score': np.float32(0.999589), 'word': 'com', 'start': 107, 'end': 110}\n",
      "{'entity_group': 'URL', 'score': np.float32(0.8312367), 'word': '/resources', 'start': 110, 'end': 120}\n",
      "{'entity_group': 'DOMAIN', 'score': np.float32(0.9985917), 'word': ' securehosting.net', 'start': 148, 'end': 165}\n",
      "{'entity_group': 'DOMAIN', 'score': np.float32(0.9983634), 'word': ' globaltrade.org', 'start': 190, 'end': 205}\n",
      "{'entity_group': 'COMPANY', 'score': np.float32(0.9151559), 'word': ' NovaSphere Industries', 'start': 243, 'end': 264}\n",
      "{'entity_group': 'COMPANY', 'score': np.float32(0.93324655), 'word': ' Zenith Solutions', 'start': 266, 'end': 282}\n",
      "{'entity_group': 'COMPANY', 'score': np.float32(0.8646218), 'word': ' Vertex Tech Labs', 'start': 284, 'end': 300}\n",
      "{'entity_group': 'DOMAIN', 'score': np.float32(0.99915063), 'word': ' datainsights.global', 'start': 323, 'end': 342}\n",
      "{'entity_group': 'DOMAIN', 'score': np.float32(0.9996834), 'word': ' techmonitor.co', 'start': 344, 'end': 358}\n",
      "{'entity_group': 'EMAIL', 'score': np.float32(0.99952006), 'word': ' clara.jensen@', 'start': 471, 'end': 484}\n",
      "{'entity_group': 'DOMAIN', 'score': np.float32(0.99989015), 'word': 'strategicvision', 'start': 484, 'end': 499}\n",
      "{'entity_group': 'DOMAIN', 'score': np.float32(0.99986565), 'word': 'com', 'start': 500, 'end': 503}\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from optimum.onnxruntime import ORTModelForTokenClassification\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "def chunk_text_with_overlap(text: str, tokenizer, max_length: int = 512, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits `text` into chunks of up to `max_length` tokens (subword tokens),\n",
    "    with `overlap` tokens overlap between consecutive chunks.\n",
    "\n",
    "    :param text: The entire text string to split.\n",
    "    :param tokenizer: The tokenizer instance to tokenize the text.\n",
    "    :param max_length: Maximum number of tokens in a chunk (e.g., 512).\n",
    "    :param overlap: Number of tokens to overlap between consecutive chunks.\n",
    "    :return: A list of text chunks.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start < len(tokens):\n",
    "        end = start + max_length\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        \n",
    "        # Convert tokens back to string\n",
    "        chunk_text = tokenizer.convert_tokens_to_string(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "\n",
    "        # Move the start pointer (sliding window approach)\n",
    "        start += max_length - overlap\n",
    "        if start < 0:\n",
    "            break\n",
    "\n",
    "    return chunks\n",
    "\n",
    "onnx_path = \"models/onnx\"\n",
    "trained_model_path = \"models/ner\"\n",
    "\n",
    "# Load the ONNX model and tokenizer\n",
    "onnx_model = ORTModelForTokenClassification.from_pretrained(onnx_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(trained_model_path)\n",
    "\n",
    "# Define the label map\n",
    "label_map = {\n",
    "    0: \"O\",\n",
    "    1: \"COMPANY\",\n",
    "    2: \"DOMAIN\",\n",
    "    3: \"IP_ADDR\",\n",
    "    4: \"URL\",\n",
    "    5: \"EMAIL\"\n",
    "}\n",
    "\n",
    "# Update the model's config with the label map\n",
    "onnx_model.config.id2label = label_map\n",
    "onnx_model.config.label2id = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# Create a pipeline for NER\n",
    "# Set device to 0 if you have a GPU, or -1 if you're using CPU\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=onnx_model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0,  # use 0 (GPU) or -1 (CPU)\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "# Read text from file \"testdoc.txt\"\n",
    "input_file = \"testdoc.txt\"\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Split the text into overlapping chunks\n",
    "# Change max_length and overlap to suit your model/context needs\n",
    "text_chunks = chunk_text_with_overlap(text, tokenizer, max_length=200, overlap=50)\n",
    "\n",
    "all_results = []\n",
    "for idx, chunk in enumerate(text_chunks):\n",
    "    print(f\"Processing chunk {idx+1}/{len(text_chunks)}...\")\n",
    "    results = ner_pipeline(chunk)\n",
    "    all_results.extend(results)\n",
    "\n",
    "for res in all_results:\n",
    "    print(res)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner-zCheM3vK-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
